{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameter\n",
    "- Attention = Luong\n",
    "- Teacher Forcing Ratio = 0.5\n",
    "- Layer = 1\n",
    "- Batch size = 32\n",
    "- Learning rate = 0.001\n",
    "- Hidden unit = 200\n",
    "- Epochs = 100\n",
    "- N = 100\n",
    "- Data Length = 100K\n",
    "- Data = [single_Ctype4, last_separator_Ctype4, single_Ctype2_concat, separator_Ctype4]\n",
    "- Deduplication\n",
    "- Random split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "\n",
    "import useful packages for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyeontae/hyeontae/venv/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/hyeontae/hyeontae/venv/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchtext\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname('__file__'))))))))\n",
    "\n",
    "from trainer.supervised_trainer_correction import SupervisedTrainer_correction\n",
    "from models.encoderRNN_posemb_add import EncoderRNN_posemb_add\n",
    "from models.decoderRNN_posemb_add import DecoderRNN_posemb_add\n",
    "from models.seq2seq import Seq2seq\n",
    "from loss.loss import Perplexity\n",
    "from optim.optim import Optimizer\n",
    "from dataset import fields\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_level = 'info'\n",
    "LOG_FORMAT = '%(asctime)s %(levelname)-6s %(message)s'\n",
    "logging.basicConfig(format=LOG_FORMAT, level=getattr(logging, log_level.upper()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = [\"single_Ctype4\"]\n",
    "character_accuracy = []\n",
    "sentence_accuracy = []\n",
    "f1_score = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data : single_Ctype4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyeontae/hyeontae/venv/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/hyeontae/hyeontae/venv/lib/python3.6/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "2019-05-26 15:24:10,832 INFO   Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      "), Scheduler: None\n",
      "/home/hyeontae/hyeontae/venv/lib/python3.6/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "for i in data_path:\n",
    "    print(\"data : %s\" % i)\n",
    "    train_path = \"../../../data/copy_rand/correction_\" + i + \"/data_train.txt\"\n",
    "    dev_path = \"../../../data/copy_rand/correction_\" + i + \"/data_test.txt\"\n",
    "\n",
    "    src = fields.SourceField()\n",
    "    tgt = fields.TargetField()\n",
    "    max_len = 104\n",
    "    def len_filter(example):\n",
    "        return len(example.src) <= max_len and len(example.tgt) <= max_len\n",
    "    train = torchtext.data.TabularDataset(\n",
    "        path=train_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )\n",
    "    dev = torchtext.data.TabularDataset(\n",
    "        path=dev_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )\n",
    "    src.build_vocab(train)\n",
    "    tgt.build_vocab(train)\n",
    "    input_vocab = src.vocab\n",
    "    output_vocab = tgt.vocab\n",
    "\n",
    "    weight = torch.ones(len(tgt.vocab))\n",
    "    pad = tgt.vocab.stoi[tgt.pad_token]\n",
    "    loss = Perplexity(weight, pad)\n",
    "    if torch.cuda.is_available():\n",
    "        loss.cuda()\n",
    "    \n",
    "    optimizer = \"Adam\"\n",
    "    hidden_size = 200\n",
    "    bidirectional = False\n",
    "\n",
    "    seq2seq = None\n",
    "    encoder = EncoderRNN_posemb_add(len(src.vocab), max_len, hidden_size,\n",
    "                         bidirectional=bidirectional, rnn_cell='lstm', variable_lengths=True)\n",
    "    decoder = DecoderRNN_posemb_add(len(tgt.vocab), max_len, hidden_size * 2 if bidirectional else hidden_size,\n",
    "                         dropout_p=0.2, use_attention=\"Luong\", bidirectional=bidirectional,rnn_cell='lstm',\n",
    "                         eos_id=tgt.eos_id, sos_id=tgt.sos_id)\n",
    "    seq2seq = Seq2seq(encoder, decoder)\n",
    "    if torch.cuda.is_available():\n",
    "        seq2seq.cuda()\n",
    "\n",
    "    for param in seq2seq.parameters():\n",
    "        param.data.uniform_(-0.08, 0.08)\n",
    "\n",
    "    # train\n",
    "    t = SupervisedTrainer_correction(loss=loss, batch_size=32,\n",
    "                          checkpoint_every=50,\n",
    "                          print_every=100,\n",
    "                          hidden_size=hidden_size,\n",
    "                          path=\"copy_rand_correction_ctype4_posemb_add\",\n",
    "                          file_name=i)\n",
    "\n",
    "    seq2seq, ave_loss, character_accuracy_list, sentence_accuracy_list, f1_score_list = t.train(seq2seq, train,\n",
    "                                                                             num_epochs=100, dev_data=dev,\n",
    "                                                                             optimizer=optimizer,\n",
    "                                                                             teacher_forcing_ratio=0.5)\n",
    "\n",
    "    character_accuracy.append(character_accuracy_list)\n",
    "    sentence_accuracy.append(sentence_accuracy_list)\n",
    "    f1_score.append(f1_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_path:\n",
    "    print(\"data : %s\" % i)\n",
    "    train_path = \"../../../data/copy_rand/correction_\" + i + \"/data_train.txt\"\n",
    "    dev_path = \"../../../data/copy_rand/correction_\" + i + \"/data_test.txt\"\n",
    "\n",
    "    src = fields.SourceField()\n",
    "    tgt = fields.TargetField()\n",
    "    max_len = 104\n",
    "    def len_filter(example):\n",
    "        return len(example.src) <= max_len and len(example.tgt) <= max_len\n",
    "    train = torchtext.data.TabularDataset(\n",
    "        path=train_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )\n",
    "    dev = torchtext.data.TabularDataset(\n",
    "        path=dev_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )\n",
    "    src.build_vocab(train)\n",
    "    tgt.build_vocab(train)\n",
    "    input_vocab = src.vocab\n",
    "    output_vocab = tgt.vocab\n",
    "\n",
    "    weight = torch.ones(len(tgt.vocab))\n",
    "    pad = tgt.vocab.stoi[tgt.pad_token]\n",
    "    loss = Perplexity(weight, pad)\n",
    "    if torch.cuda.is_available():\n",
    "        loss.cuda()\n",
    "    \n",
    "    optimizer = \"Adam\"\n",
    "    hidden_size = 200\n",
    "    bidirectional = False\n",
    "\n",
    "    seq2seq = None\n",
    "    encoder = EncoderRNN_posemb_add(len(src.vocab), max_len, hidden_size,\n",
    "                         bidirectional=bidirectional, rnn_cell='lstm', variable_lengths=True)\n",
    "    decoder = DecoderRNN_posemb_add(len(tgt.vocab), max_len, hidden_size * 2 if bidirectional else hidden_size,\n",
    "                         dropout_p=0.2, use_attention=\"Luong\", bidirectional=bidirectional,rnn_cell='lstm',\n",
    "                         eos_id=tgt.eos_id, sos_id=tgt.sos_id)\n",
    "    seq2seq = Seq2seq(encoder, decoder)\n",
    "    if torch.cuda.is_available():\n",
    "        seq2seq.cuda()\n",
    "\n",
    "    for param in seq2seq.parameters():\n",
    "        param.data.uniform_(-0.08, 0.08)\n",
    "\n",
    "    # train\n",
    "    t = SupervisedTrainer_correction(loss=loss, batch_size=32,\n",
    "                          checkpoint_every=50,\n",
    "                          print_every=100,\n",
    "                          hidden_size=hidden_size,\n",
    "                          path=\"copy_rand_correction_ctype4_posemb_add\",\n",
    "                          file_name=i)\n",
    "\n",
    "    seq2seq, ave_loss, character_accuracy_list, sentence_accuracy_list, f1_score_list = t.train(seq2seq, train,\n",
    "                                                                             num_epochs=100, dev_data=dev,\n",
    "                                                                             optimizer=optimizer,\n",
    "                                                                             teacher_forcing_ratio=0.5)\n",
    "\n",
    "    character_accuracy.append(character_accuracy_list)\n",
    "    sentence_accuracy.append(sentence_accuracy_list)\n",
    "    f1_score.append(f1_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_path:\n",
    "    print(\"data : %s\" % i)\n",
    "    train_path = \"../../../data/copy_length/correction_\" + i + \"/data_train.txt\"\n",
    "    dev_path = \"../../../data/copy_length/correction_\" + i + \"/data_test.txt\"\n",
    "\n",
    "    src = fields.SourceField()\n",
    "    tgt = fields.TargetField()\n",
    "    max_len = 104\n",
    "    def len_filter(example):\n",
    "        return len(example.src) <= max_len and len(example.tgt) <= max_len\n",
    "    train = torchtext.data.TabularDataset(\n",
    "        path=train_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )\n",
    "    dev = torchtext.data.TabularDataset(\n",
    "        path=dev_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )\n",
    "    src.build_vocab(train)\n",
    "    tgt.build_vocab(train)\n",
    "    input_vocab = src.vocab\n",
    "    output_vocab = tgt.vocab\n",
    "\n",
    "    weight = torch.ones(len(tgt.vocab))\n",
    "    pad = tgt.vocab.stoi[tgt.pad_token]\n",
    "    loss = Perplexity(weight, pad)\n",
    "    if torch.cuda.is_available():\n",
    "        loss.cuda()\n",
    "    \n",
    "    optimizer = \"Adam\"\n",
    "    hidden_size = 200\n",
    "    bidirectional = False\n",
    "\n",
    "    seq2seq = None\n",
    "    encoder = EncoderRNN_posemb_add(len(src.vocab), max_len, hidden_size,\n",
    "                         bidirectional=bidirectional, rnn_cell='lstm', variable_lengths=True)\n",
    "    decoder = DecoderRNN_posemb_add(len(tgt.vocab), max_len, hidden_size * 2 if bidirectional else hidden_size,\n",
    "                         dropout_p=0.2, use_attention=\"Luong\", bidirectional=bidirectional,rnn_cell='lstm',\n",
    "                         eos_id=tgt.eos_id, sos_id=tgt.sos_id)\n",
    "    seq2seq = Seq2seq(encoder, decoder)\n",
    "    if torch.cuda.is_available():\n",
    "        seq2seq.cuda()\n",
    "\n",
    "    for param in seq2seq.parameters():\n",
    "        param.data.uniform_(-0.08, 0.08)\n",
    "\n",
    "    # train\n",
    "    t = SupervisedTrainer_correction(loss=loss, batch_size=32,\n",
    "                          checkpoint_every=50,\n",
    "                          print_every=100,\n",
    "                          hidden_size=hidden_size,\n",
    "                          path=\"copy_length_correction_ctype4_posemb_add\",\n",
    "                          file_name=i)\n",
    "\n",
    "    seq2seq, ave_loss, character_accuracy_list, sentence_accuracy_list, f1_score_list = t.train(seq2seq, train,\n",
    "                                                                             num_epochs=100, dev_data=dev,\n",
    "                                                                             optimizer=optimizer,\n",
    "                                                                             teacher_forcing_ratio=0.5)\n",
    "\n",
    "    character_accuracy.append(character_accuracy_list)\n",
    "    sentence_accuracy.append(sentence_accuracy_list)\n",
    "    f1_score.append(f1_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(1, 101, 1))\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(epochs[::3], sentence_accuracy[0][::3], '-', LineWidth=3, label=\"LSTM PosEmb Add 100N\")\n",
    "plt.plot(epochs[::3], sentence_accuracy[1][::3], '-s', LineWidth=3, label=\"LSTM PosEmb Add 50N\")\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=24)\n",
    "plt.ylabel('Sentence Accuracy', fontsize=24)\n",
    "plt.ylim([0, 1])\n",
    "#plt.savefig('../../../log/plot/gru_palindrome_rand_correction_ctype4_lenemb_add_order_encoder_decoder/ctype_to_sentence_accuracy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(epochs[::3], f1_score[0][::3], '-', LineWidth=3, label=\"LSTM PosEmb Add 100N\")\n",
    "plt.plot(epochs[::3], f1_score[1][::3], '-s', LineWidth=3, label=\"LSTM PosEmb Add 50N\")\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=24)\n",
    "plt.ylabel('F1 Score', fontsize=24)\n",
    "plt.ylim([0, 1])\n",
    "#plt.savefig('../../../log/plot/gru_palindrome_rand_correction_ctype4_lenemb_add_order_encoder_decoder/ctype_to_f1_score.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate_list = []\n",
    "for i in range(len(character_accuracy)):\n",
    "    error_rate = []\n",
    "    for j in character_accuracy[i]:\n",
    "        error_rate.append(1 - j)\n",
    "    error_rate_list.append(error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(1, 101, 1))\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(epochs[::3], error_rate_list[0][::3], '-', LineWidth=3, label=\"LSTM PosEmb Add 100N\")\n",
    "plt.plot(epochs[::3], error_rate_list[1][::3], '-s', LineWidth=3, label=\"LSTM PosEmb Add 50N\")\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=24)\n",
    "plt.ylabel('Error Rate', fontsize=24)\n",
    "plt.yscale('log')\n",
    "#plt.ylim([0, 1])\n",
    "#plt.savefig('../../../log/plot/gru_palindrome_rand_correction_ctype4_lenemb_add_order_encoder_decoder/ctype_to_error_rate.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(character_accuracy[0])\n",
    "print(character_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sentence_accuracy[0])\n",
    "print(sentence_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f1_score[0])\n",
    "print(f1_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

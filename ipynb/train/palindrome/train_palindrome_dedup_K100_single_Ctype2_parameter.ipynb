{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameter\n",
    "- Attention = [None, Luong]\n",
    "- Teacher Forcing Ratio = 0.5\n",
    "- Layer = 1\n",
    "- Batch size = 32\n",
    "- Drop out = 0.2\n",
    "- Hidden unit = 50\n",
    "- Epochs = 100\n",
    "- N = 100\n",
    "- Data Length = 100K\n",
    "- Single\n",
    "- Cype = 2\n",
    "- Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "\n",
    "import useful packages for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyeontae/hyeontae/venv2/lib/python3.5/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/hyeontae/hyeontae/venv2/lib/python3.5/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchtext\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname('__file__'))))))))\n",
    "\n",
    "from trainer.supervised_trainer import SupervisedTrainer\n",
    "from models.encoderRNN import EncoderRNN\n",
    "from models.decoderRNN import DecoderRNN\n",
    "from models.seq2seq import Seq2seq\n",
    "from loss.loss import Perplexity\n",
    "from optim.optim import Optimizer\n",
    "from dataset import fields\n",
    "from evaluator.predictor import Predictor\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_level = 'info'\n",
    "LOG_FORMAT = '%(asctime)s %(levelname)-6s %(message)s'\n",
    "logging.basicConfig(format=LOG_FORMAT, level=getattr(logging, log_level.upper()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_accuracy = []\n",
    "sentence_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyeontae/hyeontae/venv2/lib/python3.5/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "train_path = \"../../../data/palindrome_dedup/K100_single_Ctype2/data_train.txt\"\n",
    "dev_path = \"../../../data/palindrome_dedup/K100_single_Ctype2/data_test.txt\"\n",
    "\n",
    "src = fields.SourceField()\n",
    "tgt = fields.TargetField()\n",
    "max_len = 104\n",
    "def len_filter(example):\n",
    "    return len(example.src) <= max_len and len(example.tgt) <= max_len\n",
    "train = torchtext.data.TabularDataset(\n",
    "    path=train_path, format='tsv',\n",
    "    fields=[('src', src), ('tgt', tgt)],\n",
    "    filter_pred=len_filter\n",
    ")\n",
    "dev = torchtext.data.TabularDataset(\n",
    "    path=dev_path, format='tsv',\n",
    "    fields=[('src', src), ('tgt', tgt)],\n",
    "    filter_pred=len_filter\n",
    ")\n",
    "src.build_vocab(train)\n",
    "tgt.build_vocab(train)\n",
    "input_vocab = src.vocab\n",
    "output_vocab = tgt.vocab\n",
    "\n",
    "weight = torch.ones(len(tgt.vocab))\n",
    "pad = tgt.vocab.stoi[tgt.pad_token]\n",
    "loss = Perplexity(weight, pad)\n",
    "if torch.cuda.is_available():\n",
    "    loss.cuda()\n",
    "    \n",
    "optimizer = \"Adam\"\n",
    "hidden_size = 50\n",
    "bidirectional = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyeontae/hyeontae/venv2/lib/python3.5/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "2019-04-04 16:27:16,179 INFO   Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      "), Scheduler: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luong Att\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyeontae/hyeontae/venv2/lib/python3.5/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "2019-04-04 16:30:26,984 INFO   Finished epoch 1: Train loss: 14.2942, Dev loss: 14.2104, Accuracy(Character): 0.4304, Accuracy(Word): 0.0161\n",
      "2019-04-04 16:33:38,162 INFO   Finished epoch 2: Train loss: 5.3854, Dev loss: 6.4352, Accuracy(Character): 0.5824, Accuracy(Word): 0.0091\n",
      "2019-04-04 16:36:53,694 INFO   Finished epoch 3: Train loss: 4.6129, Dev loss: 5.8984, Accuracy(Character): 0.5305, Accuracy(Word): 0.0003\n",
      "2019-04-04 16:40:04,254 INFO   Finished epoch 4: Train loss: 4.6873, Dev loss: 9.4771, Accuracy(Character): 0.5301, Accuracy(Word): 0.0026\n",
      "2019-04-04 16:43:13,481 INFO   Finished epoch 5: Train loss: 5.8628, Dev loss: 22.9286, Accuracy(Character): 0.4892, Accuracy(Word): 0.0094\n",
      "2019-04-04 16:46:25,648 INFO   Finished epoch 6: Train loss: 6.7609, Dev loss: 17.8463, Accuracy(Character): 0.4773, Accuracy(Word): 0.0013\n",
      "2019-04-04 16:49:35,206 INFO   Finished epoch 7: Train loss: 8.4099, Dev loss: 31.2310, Accuracy(Character): 0.4766, Accuracy(Word): 0.0091\n",
      "2019-04-04 16:53:12,510 INFO   Finished epoch 8: Train loss: 9.0655, Dev loss: 18.5262, Accuracy(Character): 0.4917, Accuracy(Word): 0.0000\n",
      "2019-04-04 16:56:59,702 INFO   Finished epoch 9: Train loss: 7.4049, Dev loss: 29.2566, Accuracy(Character): 0.4896, Accuracy(Word): 0.0000\n",
      "2019-04-04 17:00:49,533 INFO   Finished epoch 10: Train loss: 9.7971, Dev loss: 42.5599, Accuracy(Character): 0.4961, Accuracy(Word): 0.0000\n",
      "2019-04-04 17:04:36,131 INFO   Finished epoch 11: Train loss: 9.4222, Dev loss: 21.9959, Accuracy(Character): 0.4848, Accuracy(Word): 0.0000\n",
      "2019-04-04 17:08:25,770 INFO   Finished epoch 12: Train loss: 9.9400, Dev loss: 43.6837, Accuracy(Character): 0.5444, Accuracy(Word): 0.0000\n",
      "2019-04-04 17:12:15,604 INFO   Finished epoch 13: Train loss: 7.9421, Dev loss: 41.3058, Accuracy(Character): 0.4779, Accuracy(Word): 0.0000\n",
      "2019-04-04 17:16:30,682 INFO   Finished epoch 14: Train loss: 13.2089, Dev loss: 50.6276, Accuracy(Character): 0.4767, Accuracy(Word): 0.0050\n",
      "2019-04-04 17:20:19,208 INFO   Finished epoch 15: Train loss: 10.9030, Dev loss: 65.2399, Accuracy(Character): 0.4822, Accuracy(Word): 0.0000\n",
      "2019-04-04 17:24:45,450 INFO   Finished epoch 16: Train loss: 8.3678, Dev loss: 89.5243, Accuracy(Character): 0.4827, Accuracy(Word): 0.0000\n",
      "2019-04-04 17:29:32,127 INFO   Finished epoch 17: Train loss: 9.8400, Dev loss: 47.2025, Accuracy(Character): 0.4876, Accuracy(Word): 0.0063\n",
      "2019-04-04 17:34:12,342 INFO   Finished epoch 18: Train loss: 8.7305, Dev loss: 107.6182, Accuracy(Character): 0.4941, Accuracy(Word): 0.0000\n",
      "2019-04-04 17:39:18,927 INFO   Finished epoch 19: Train loss: 10.1941, Dev loss: 72.9687, Accuracy(Character): 0.4874, Accuracy(Word): 0.0000\n",
      "2019-04-04 17:43:49,658 INFO   Finished epoch 20: Train loss: 9.8253, Dev loss: 123.5893, Accuracy(Character): 0.4983, Accuracy(Word): 0.0000\n",
      "2019-04-04 17:48:16,839 INFO   Finished epoch 21: Train loss: 7.7792, Dev loss: 76.7721, Accuracy(Character): 0.4790, Accuracy(Word): 0.0000\n",
      "2019-04-04 17:52:43,009 INFO   Finished epoch 22: Train loss: 7.5323, Dev loss: 127.6040, Accuracy(Character): 0.4900, Accuracy(Word): 0.0000\n",
      "2019-04-04 17:57:07,495 INFO   Finished epoch 23: Train loss: 10.7122, Dev loss: 153.9486, Accuracy(Character): 0.4882, Accuracy(Word): 0.0009\n",
      "2019-04-04 18:01:35,344 INFO   Finished epoch 24: Train loss: 7.8054, Dev loss: 131.1603, Accuracy(Character): 0.4771, Accuracy(Word): 0.0004\n",
      "2019-04-04 18:06:09,547 INFO   Finished epoch 25: Train loss: 16.3786, Dev loss: 176.2265, Accuracy(Character): 0.4766, Accuracy(Word): 0.0013\n",
      "2019-04-04 18:10:39,424 INFO   Finished epoch 26: Train loss: 19.5064, Dev loss: 441.3848, Accuracy(Character): 0.4758, Accuracy(Word): 0.0063\n",
      "2019-04-04 18:15:07,322 INFO   Finished epoch 27: Train loss: 22.5586, Dev loss: 217.1886, Accuracy(Character): 0.4818, Accuracy(Word): 0.0000\n",
      "2019-04-04 18:19:34,377 INFO   Finished epoch 28: Train loss: 12.0025, Dev loss: 685.7782, Accuracy(Character): 0.4845, Accuracy(Word): 0.0050\n",
      "2019-04-04 18:24:03,187 INFO   Finished epoch 29: Train loss: 20.2604, Dev loss: 459.2730, Accuracy(Character): 0.4894, Accuracy(Word): 0.0000\n",
      "2019-04-04 18:28:32,907 INFO   Finished epoch 30: Train loss: 21.8040, Dev loss: 643.1105, Accuracy(Character): 0.4774, Accuracy(Word): 0.0000\n",
      "2019-04-04 18:33:04,384 INFO   Finished epoch 31: Train loss: 17.2080, Dev loss: 119.2355, Accuracy(Character): 0.5234, Accuracy(Word): 0.0000\n",
      "2019-04-04 18:37:30,273 INFO   Finished epoch 32: Train loss: 16.2973, Dev loss: 808.1241, Accuracy(Character): 0.4740, Accuracy(Word): 0.0000\n",
      "2019-04-04 18:41:49,753 INFO   Finished epoch 33: Train loss: 27.0409, Dev loss: 682.5547, Accuracy(Character): 0.4846, Accuracy(Word): 0.0094\n",
      "2019-04-04 18:45:27,169 INFO   Finished epoch 34: Train loss: 19.9310, Dev loss: 268.8959, Accuracy(Character): 0.4746, Accuracy(Word): 0.0000\n",
      "2019-04-04 18:49:09,809 INFO   Finished epoch 35: Train loss: 15.7317, Dev loss: 740.9782, Accuracy(Character): 0.5014, Accuracy(Word): 0.0013\n",
      "2019-04-04 18:53:02,453 INFO   Finished epoch 36: Train loss: 21.6035, Dev loss: 250.1216, Accuracy(Character): 0.4835, Accuracy(Word): 0.0013\n",
      "2019-04-04 18:56:49,311 INFO   Finished epoch 37: Train loss: 23.3420, Dev loss: 205.6490, Accuracy(Character): 0.4868, Accuracy(Word): 0.0013\n",
      "2019-04-04 19:00:30,975 INFO   Finished epoch 38: Train loss: 11.9048, Dev loss: 418.5779, Accuracy(Character): 0.4776, Accuracy(Word): 0.0000\n",
      "2019-04-04 19:04:12,495 INFO   Finished epoch 39: Train loss: 13.3989, Dev loss: 490.3955, Accuracy(Character): 0.4840, Accuracy(Word): 0.0013\n",
      "2019-04-04 19:07:51,513 INFO   Finished epoch 40: Train loss: 10.1373, Dev loss: 672.8479, Accuracy(Character): 0.4803, Accuracy(Word): 0.0000\n",
      "2019-04-04 19:11:32,445 INFO   Finished epoch 41: Train loss: 15.4326, Dev loss: 611.6523, Accuracy(Character): 0.4724, Accuracy(Word): 0.0000\n",
      "2019-04-04 19:15:13,763 INFO   Finished epoch 42: Train loss: 16.6783, Dev loss: 1165.7851, Accuracy(Character): 0.4796, Accuracy(Word): 0.0000\n",
      "2019-04-04 19:18:55,678 INFO   Finished epoch 43: Train loss: 19.9088, Dev loss: 702.3041, Accuracy(Character): 0.4762, Accuracy(Word): 0.0000\n",
      "2019-04-04 19:22:40,688 INFO   Finished epoch 44: Train loss: 21.9380, Dev loss: 693.4419, Accuracy(Character): 0.4727, Accuracy(Word): 0.0000\n",
      "2019-04-04 19:26:24,174 INFO   Finished epoch 45: Train loss: 9.1752, Dev loss: 1932.7655, Accuracy(Character): 0.4844, Accuracy(Word): 0.0000\n",
      "2019-04-04 19:30:06,277 INFO   Finished epoch 46: Train loss: 17.1509, Dev loss: 1875.7955, Accuracy(Character): 0.4719, Accuracy(Word): 0.0000\n",
      "2019-04-04 19:33:47,901 INFO   Finished epoch 47: Train loss: 21.8254, Dev loss: 429.4906, Accuracy(Character): 0.4801, Accuracy(Word): 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f019c290772b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                                                              \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                                                              \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                                                                              teacher_forcing_ratio=0.5)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mcharacter_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharacter_accuracy_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hyeontae/Context-Free_Grammar/trainer/supervised_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, data, num_epochs, resume, dev_data, optimizer, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    224\u001b[0m         loss, character_accuracy_list, sentance_accuracy_list = self._train_epoches(data, model, num_epochs,\n\u001b[1;32m    225\u001b[0m                                                                 \u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                                                                 teacher_forcing_ratio=teacher_forcing_ratio)\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharacter_accuracy_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentance_accuracy_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hyeontae/Context-Free_Grammar/trainer/supervised_trainer.py\u001b[0m in \u001b[0;36m_train_epoches\u001b[0;34m(self, data, model, n_epochs, start_epoch, start_step, dev_data, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdev_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mdev_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharacter_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentance_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mlog_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\", Dev %s: %.4f, Accuracy(Character): %.4f, Accuracy(Word): %.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharacter_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentance_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hyeontae/Context-Free_Grammar/evaluator/evaluator.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, model, data)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mtarget_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tgt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0mcorrect_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hyeontae/venv2/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hyeontae/Context-Free_Grammar/models/seq2seq.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_variable, input_lengths, target_variable, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     21\u001b[0m                               \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                               \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                               teacher_forcing_ratio=teacher_forcing_ratio)\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hyeontae/venv2/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hyeontae/Context-Free_Grammar/models/decoderRNN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, encoder_hidden, encoder_outputs, function, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    157\u001b[0m                     \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0msymbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m                 \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msymbols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hyeontae/Context-Free_Grammar/models/decoderRNN.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(step, step_output, step_attn)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0meos_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0meos_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meos_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                 \u001b[0mupdate_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0meos_batches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_symbols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msymbols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Luong Att\")\n",
    "\n",
    "seq2seq = None\n",
    "encoder = EncoderRNN(len(src.vocab), max_len, hidden_size,\n",
    "                    bidirectional=bidirectional, variable_lengths=True)\n",
    "decoder = DecoderRNN(len(tgt.vocab), max_len, hidden_size * 2 if bidirectional else hidden_size,\n",
    "                     dropout_p=0.2, use_attention=\"Luong\", bidirectional=bidirectional,\n",
    "                     eos_id=tgt.eos_id, sos_id=tgt.sos_id)\n",
    "seq2seq = Seq2seq(encoder, decoder)\n",
    "if torch.cuda.is_available():\n",
    "    seq2seq.cuda()\n",
    "\n",
    "for param in seq2seq.parameters():\n",
    "    param.data.uniform_(-0.08, 0.08)\n",
    "\n",
    "# train\n",
    "t = SupervisedTrainer(loss=loss, batch_size=32,\n",
    "                      checkpoint_every=50,\n",
    "                      print_every=100,\n",
    "                      hidden_size=hidden_size,\n",
    "                      path=\"palindrome_dedup_K100_single_Ctype2_parameter/Luong_att\")\n",
    "\n",
    "seq2seq, ave_loss, character_accuracy_list, sentence_accuracy_list = t.train(seq2seq, train,\n",
    "                                                                             num_epochs=100, dev_data=dev,\n",
    "                                                                             optimizer=optimizer,\n",
    "                                                                             teacher_forcing_ratio=0.5)\n",
    "\n",
    "character_accuracy.append(character_accuracy_list)\n",
    "sentence_accuracy.append(sentence_accuracy_list)\n",
    "\n",
    "torch.save(seq2seq.state_dict(), '../../../log/pth/palindrome_dedup_K100_single_Ctype2_parameter_Luong_att_model_save.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"None Att\")\n",
    "\n",
    "seq2seq = None\n",
    "encoder = EncoderRNN(len(src.vocab), max_len, hidden_size,\n",
    "                    bidirectional=bidirectional, variable_lengths=True)\n",
    "decoder = DecoderRNN(len(tgt.vocab), max_len, hidden_size * 2 if bidirectional else hidden_size,\n",
    "                     dropout_p=0.2, use_attention=None, bidirectional=bidirectional,\n",
    "                     eos_id=tgt.eos_id, sos_id=tgt.sos_id)\n",
    "seq2seq = Seq2seq(encoder, decoder)\n",
    "if torch.cuda.is_available():\n",
    "    seq2seq.cuda()\n",
    "\n",
    "for param in seq2seq.parameters():\n",
    "    param.data.uniform_(-0.08, 0.08)\n",
    "\n",
    "# train\n",
    "t = SupervisedTrainer(loss=loss, batch_size=32,\n",
    "                      checkpoint_every=50,\n",
    "                      print_every=100,\n",
    "                      hidden_size=hidden_size,\n",
    "                      path=\"palindrome_dedup_K100_single_Ctype2_parameter/None_att\")\n",
    "\n",
    "seq2seq, ave_loss, character_accuracy_list, sentence_accuracy_list = t.train(seq2seq, train,\n",
    "                                                                             num_epochs=100, dev_data=dev,\n",
    "                                                                             optimizer=optimizer,\n",
    "                                                                             teacher_forcing_ratio=0.5)\n",
    "\n",
    "character_accuracy.append(character_accuracy_list)\n",
    "sentence_accuracy.append(sentence_accuracy_list)\n",
    "\n",
    "torch.save(seq2seq.state_dict(), '../../../log/pth/palindrome_dedup_K100_single_Ctype2_parameter_no_att_model_save.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = list(range(1, 101, 1))\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(epochs[::3], character_accuracy[0][::3], '--', LineWidth=3, label=\"Luong Att\"\")\n",
    "plt.plot(epochs[::3], character_accuracy[1][::3], '-o', LineWidth=3, label=\"None Att\")\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=24)\n",
    "plt.ylabel('Character Accuracy', fontsize=24)\n",
    "plt.ylim([0, 1])\n",
    "plt.title('palindrome Ctype2', fontsize=35, fontweight=560)\n",
    "plt.savefig('../../../log/plot/palindrome_dedup_K100_single_Ctype2_parameter/epoch_to_character_accuracy.png')\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(epochs[::3], sentence_accuracy[0][::3], '--', LineWidth=3, label=\"Luong Att\")\n",
    "plt.plot(epochs[::3], sentence_accuracy[1][::3], '-o', LineWidth=3, label=\"None Att\")\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=24)\n",
    "plt.ylabel('Sentence Accuracy', fontsize=24)\n",
    "plt.ylim([0, 1])\n",
    "plt.title('palindrome Ctype2', fontsize=35, fontweight=560)\n",
    "plt.savefig('../../../log/plot/palindrome_dedup_K100_single_Ctype2_parameter/epoch_to_sentence_accuracy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(character_accuracy[0])\n",
    "print(character_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sentence_accuracy[0])\n",
    "print(sentence_accuracy[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

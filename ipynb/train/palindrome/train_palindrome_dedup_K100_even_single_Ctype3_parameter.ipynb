{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameter\n",
    "- Attention = [None, Luong]\n",
    "- Teacher Forcing Ratio = 0.5\n",
    "- Layer = 1\n",
    "- Batch size = 32\n",
    "- Drop out = 0.2\n",
    "- Hidden unit = 50\n",
    "- Epochs = 100\n",
    "- N = 100\n",
    "- Data Length = 100K\n",
    "- Single\n",
    "- Even\n",
    "- Cype = 3\n",
    "- Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "\n",
    "import useful packages for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyeontae/hyeontae/venv2/lib/python3.5/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/hyeontae/hyeontae/venv2/lib/python3.5/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchtext\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname('__file__'))))))))\n",
    "\n",
    "from trainer.supervised_trainer import SupervisedTrainer\n",
    "from models.encoderRNN import EncoderRNN\n",
    "from models.decoderRNN import DecoderRNN\n",
    "from models.seq2seq import Seq2seq\n",
    "from loss.loss import Perplexity\n",
    "from optim.optim import Optimizer\n",
    "from dataset import fields\n",
    "from evaluator.predictor import Predictor\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_level = 'info'\n",
    "LOG_FORMAT = '%(asctime)s %(levelname)-6s %(message)s'\n",
    "logging.basicConfig(format=LOG_FORMAT, level=getattr(logging, log_level.upper()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_accuracy = []\n",
    "sentence_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyeontae/hyeontae/venv2/lib/python3.5/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "train_path = \"../../../data/palindrome_dedup/K100_even_single_Ctype3/data_train.txt\"\n",
    "dev_path = \"../../../data/palindrome_dedup/K100_even_single_Ctype3/data_test.txt\"\n",
    "\n",
    "src = fields.SourceField()\n",
    "tgt = fields.TargetField()\n",
    "max_len = 104\n",
    "def len_filter(example):\n",
    "    return len(example.src) <= max_len and len(example.tgt) <= max_len\n",
    "train = torchtext.data.TabularDataset(\n",
    "    path=train_path, format='tsv',\n",
    "    fields=[('src', src), ('tgt', tgt)],\n",
    "    filter_pred=len_filter\n",
    ")\n",
    "dev = torchtext.data.TabularDataset(\n",
    "    path=dev_path, format='tsv',\n",
    "    fields=[('src', src), ('tgt', tgt)],\n",
    "    filter_pred=len_filter\n",
    ")\n",
    "src.build_vocab(train)\n",
    "tgt.build_vocab(train)\n",
    "input_vocab = src.vocab\n",
    "output_vocab = tgt.vocab\n",
    "\n",
    "weight = torch.ones(len(tgt.vocab))\n",
    "pad = tgt.vocab.stoi[tgt.pad_token]\n",
    "loss = Perplexity(weight, pad)\n",
    "if torch.cuda.is_available():\n",
    "    loss.cuda()\n",
    "    \n",
    "optimizer = \"Adam\"\n",
    "hidden_size = 50\n",
    "bidirectional = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyeontae/hyeontae/venv2/lib/python3.5/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "2019-04-04 20:09:12,614 INFO   Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      "), Scheduler: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luong Att\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyeontae/hyeontae/venv2/lib/python3.5/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "2019-04-04 20:09:27,912 INFO   Finished epoch 1: Train loss: 31.5845, Dev loss: 22.3346, Accuracy(Character): 0.2021, Accuracy(Word): 0.0000\n",
      "2019-04-04 20:09:42,686 INFO   Finished epoch 2: Train loss: 16.8862, Dev loss: 17.3853, Accuracy(Character): 0.2452, Accuracy(Word): 0.0000\n",
      "2019-04-04 20:09:57,842 INFO   Finished epoch 3: Train loss: 14.2318, Dev loss: 15.6917, Accuracy(Character): 0.3022, Accuracy(Word): 0.0000\n",
      "2019-04-04 20:10:12,639 INFO   Finished epoch 4: Train loss: 12.0694, Dev loss: 16.5987, Accuracy(Character): 0.3236, Accuracy(Word): 0.0000\n",
      "2019-04-04 20:10:27,539 INFO   Finished epoch 5: Train loss: 9.9101, Dev loss: 23.7326, Accuracy(Character): 0.3080, Accuracy(Word): 0.0000\n",
      "2019-04-04 20:10:42,295 INFO   Finished epoch 6: Train loss: 11.7062, Dev loss: 12.2703, Accuracy(Character): 0.4042, Accuracy(Word): 0.0000\n",
      "2019-04-04 20:10:57,156 INFO   Finished epoch 7: Train loss: 9.4268, Dev loss: 16.6297, Accuracy(Character): 0.4355, Accuracy(Word): 0.0419\n",
      "2019-04-04 20:11:12,342 INFO   Finished epoch 8: Train loss: 9.9390, Dev loss: 11.2706, Accuracy(Character): 0.4492, Accuracy(Word): 0.0000\n",
      "2019-04-04 20:11:27,245 INFO   Finished epoch 9: Train loss: 8.8893, Dev loss: 13.2309, Accuracy(Character): 0.5097, Accuracy(Word): 0.0690\n",
      "2019-04-04 20:11:42,528 INFO   Finished epoch 10: Train loss: 6.2363, Dev loss: 10.1419, Accuracy(Character): 0.3796, Accuracy(Word): 0.0000\n",
      "2019-04-04 20:11:58,212 INFO   Finished epoch 11: Train loss: 6.1937, Dev loss: 8.2919, Accuracy(Character): 0.4997, Accuracy(Word): 0.0000\n",
      "2019-04-04 20:12:13,378 INFO   Finished epoch 12: Train loss: 4.9580, Dev loss: 8.5966, Accuracy(Character): 0.4754, Accuracy(Word): 0.0000\n",
      "2019-04-04 20:12:28,308 INFO   Finished epoch 13: Train loss: 5.3983, Dev loss: 9.7971, Accuracy(Character): 0.4711, Accuracy(Word): 0.0419\n",
      "2019-04-04 20:12:43,707 INFO   Finished epoch 14: Train loss: 4.9867, Dev loss: 10.3669, Accuracy(Character): 0.4884, Accuracy(Word): 0.0000\n",
      "2019-04-04 20:12:58,872 INFO   Finished epoch 15: Train loss: 5.1295, Dev loss: 11.9273, Accuracy(Character): 0.5309, Accuracy(Word): 0.0000\n",
      "2019-04-04 20:13:13,822 INFO   Finished epoch 16: Train loss: 4.0197, Dev loss: 10.2688, Accuracy(Character): 0.5278, Accuracy(Word): 0.0000\n",
      "2019-04-04 20:13:28,885 INFO   Finished epoch 17: Train loss: 4.5200, Dev loss: 8.5610, Accuracy(Character): 0.5206, Accuracy(Word): 0.0000\n",
      "2019-04-04 20:13:44,245 INFO   Finished epoch 18: Train loss: 5.2036, Dev loss: 14.7346, Accuracy(Character): 0.4819, Accuracy(Word): 0.0000\n",
      "2019-04-04 20:13:59,982 INFO   Finished epoch 19: Train loss: 3.7233, Dev loss: 10.6472, Accuracy(Character): 0.5245, Accuracy(Word): 0.0053\n",
      "2019-04-04 20:14:15,044 INFO   Finished epoch 20: Train loss: 6.6717, Dev loss: 13.1231, Accuracy(Character): 0.5051, Accuracy(Word): 0.0000\n",
      "2019-04-04 20:14:30,439 INFO   Finished epoch 21: Train loss: 3.6875, Dev loss: 15.0658, Accuracy(Character): 0.4912, Accuracy(Word): 0.0000\n",
      "2019-04-04 20:14:45,593 INFO   Finished epoch 22: Train loss: 5.4146, Dev loss: 11.8136, Accuracy(Character): 0.5166, Accuracy(Word): 0.0000\n",
      "2019-04-04 20:15:00,479 INFO   Finished epoch 23: Train loss: 5.6531, Dev loss: 16.8215, Accuracy(Character): 0.4959, Accuracy(Word): 0.0053\n",
      "2019-04-04 20:15:15,668 INFO   Finished epoch 24: Train loss: 3.5936, Dev loss: 16.9475, Accuracy(Character): 0.4967, Accuracy(Word): 0.0039\n",
      "2019-04-04 20:15:31,375 INFO   Finished epoch 25: Train loss: 7.5328, Dev loss: 21.0816, Accuracy(Character): 0.5022, Accuracy(Word): 0.0000\n",
      "2019-04-04 20:15:46,896 INFO   Finished epoch 26: Train loss: 3.3342, Dev loss: 18.4424, Accuracy(Character): 0.5697, Accuracy(Word): 0.0053\n",
      "2019-04-04 20:16:01,913 INFO   Finished epoch 27: Train loss: 3.6400, Dev loss: 24.3073, Accuracy(Character): 0.4999, Accuracy(Word): 0.0053\n",
      "2019-04-04 20:16:16,686 INFO   Finished epoch 28: Train loss: 2.4811, Dev loss: 32.3541, Accuracy(Character): 0.4944, Accuracy(Word): 0.0000\n",
      "2019-04-04 20:16:32,378 INFO   Finished epoch 29: Train loss: 3.9140, Dev loss: 23.4398, Accuracy(Character): 0.5265, Accuracy(Word): 0.0053\n",
      "2019-04-04 20:16:48,181 INFO   Finished epoch 30: Train loss: 3.3411, Dev loss: 21.9489, Accuracy(Character): 0.5557, Accuracy(Word): 0.0483\n",
      "2019-04-04 20:17:03,764 INFO   Finished epoch 31: Train loss: 2.9666, Dev loss: 24.3261, Accuracy(Character): 0.4924, Accuracy(Word): 0.0053\n",
      "2019-04-04 20:17:19,814 INFO   Finished epoch 32: Train loss: 2.5242, Dev loss: 28.4695, Accuracy(Character): 0.5052, Accuracy(Word): 0.0053\n",
      "2019-04-04 20:17:35,407 INFO   Finished epoch 33: Train loss: 2.4041, Dev loss: 32.6989, Accuracy(Character): 0.4924, Accuracy(Word): 0.0053\n",
      "2019-04-04 20:17:50,868 INFO   Finished epoch 34: Train loss: 6.9919, Dev loss: 28.4488, Accuracy(Character): 0.5428, Accuracy(Word): 0.0053\n",
      "2019-04-04 20:18:05,845 INFO   Finished epoch 35: Train loss: 4.3587, Dev loss: 32.6699, Accuracy(Character): 0.5149, Accuracy(Word): 0.0053\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9286bcf42ee1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                                                              \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                                                              \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                                                                              teacher_forcing_ratio=0.5)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mcharacter_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharacter_accuracy_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hyeontae/Context-Free_Grammar/trainer/supervised_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, data, num_epochs, resume, dev_data, optimizer, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    224\u001b[0m         loss, character_accuracy_list, sentance_accuracy_list = self._train_epoches(data, model, num_epochs,\n\u001b[1;32m    225\u001b[0m                                                                 \u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                                                                 teacher_forcing_ratio=teacher_forcing_ratio)\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharacter_accuracy_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentance_accuracy_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hyeontae/Context-Free_Grammar/trainer/supervised_trainer.py\u001b[0m in \u001b[0;36m_train_epoches\u001b[0;34m(self, data, model, n_epochs, start_epoch, start_step, dev_data, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdev_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mdev_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharacter_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentance_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mlog_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\", Dev %s: %.4f, Accuracy(Character): %.4f, Accuracy(Word): %.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharacter_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentance_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hyeontae/Context-Free_Grammar/evaluator/evaluator.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, model, data)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mtarget_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tgt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0mcorrect_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hyeontae/venv2/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hyeontae/Context-Free_Grammar/models/seq2seq.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_variable, input_lengths, target_variable, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     21\u001b[0m                               \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                               \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                               teacher_forcing_ratio=teacher_forcing_ratio)\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hyeontae/venv2/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hyeontae/Context-Free_Grammar/models/decoderRNN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, encoder_hidden, encoder_outputs, function, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_step_BahdanauAtt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                     \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m                 \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0msymbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hyeontae/Context-Free_Grammar/models/decoderRNN.py\u001b[0m in \u001b[0;36mforward_step\u001b[0;34m(self, input_var, hidden, encoder_outputs, function)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hyeontae/venv2/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hyeontae/venv2/lib/python3.5/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Luong Att\")\n",
    "\n",
    "seq2seq = None\n",
    "encoder = EncoderRNN(len(src.vocab), max_len, hidden_size,\n",
    "                    bidirectional=bidirectional, variable_lengths=True)\n",
    "decoder = DecoderRNN(len(tgt.vocab), max_len, hidden_size * 2 if bidirectional else hidden_size,\n",
    "                     dropout_p=0.2, use_attention=\"Luong\", bidirectional=bidirectional,\n",
    "                     eos_id=tgt.eos_id, sos_id=tgt.sos_id)\n",
    "seq2seq = Seq2seq(encoder, decoder)\n",
    "if torch.cuda.is_available():\n",
    "    seq2seq.cuda()\n",
    "\n",
    "for param in seq2seq.parameters():\n",
    "    param.data.uniform_(-0.08, 0.08)\n",
    "\n",
    "# train\n",
    "t = SupervisedTrainer(loss=loss, batch_size=32,\n",
    "                      checkpoint_every=50,\n",
    "                      print_every=100,\n",
    "                      hidden_size=hidden_size,\n",
    "                      path=\"palindrome_dedup_K100_even_single_Ctype3_parameter/Luong_att\")\n",
    "\n",
    "seq2seq, ave_loss, character_accuracy_list, sentence_accuracy_list = t.train(seq2seq, train,\n",
    "                                                                             num_epochs=100, dev_data=dev,\n",
    "                                                                             optimizer=optimizer,\n",
    "                                                                             teacher_forcing_ratio=0.5)\n",
    "\n",
    "character_accuracy.append(character_accuracy_list)\n",
    "sentence_accuracy.append(sentence_accuracy_list)\n",
    "\n",
    "torch.save(seq2seq.state_dict(), '../../../log/pth/palindrome_dedup_K100_even_single_Ctype3_parameter_Luong_att_model_save.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"None Att\")\n",
    "\n",
    "seq2seq = None\n",
    "encoder = EncoderRNN(len(src.vocab), max_len, hidden_size,\n",
    "                    bidirectional=bidirectional, variable_lengths=True)\n",
    "decoder = DecoderRNN(len(tgt.vocab), max_len, hidden_size * 2 if bidirectional else hidden_size,\n",
    "                     dropout_p=0.2, use_attention=None, bidirectional=bidirectional,\n",
    "                     eos_id=tgt.eos_id, sos_id=tgt.sos_id)\n",
    "seq2seq = Seq2seq(encoder, decoder)\n",
    "if torch.cuda.is_available():\n",
    "    seq2seq.cuda()\n",
    "\n",
    "for param in seq2seq.parameters():\n",
    "    param.data.uniform_(-0.08, 0.08)\n",
    "\n",
    "# train\n",
    "t = SupervisedTrainer(loss=loss, batch_size=32,\n",
    "                      checkpoint_every=50,\n",
    "                      print_every=100,\n",
    "                      hidden_size=hidden_size,\n",
    "                      path=\"palindrome_dedup_K100_even_single_Ctype3_parameter/None_att\")\n",
    "\n",
    "seq2seq, ave_loss, character_accuracy_list, sentence_accuracy_list = t.train(seq2seq, train,\n",
    "                                                                             num_epochs=100, dev_data=dev,\n",
    "                                                                             optimizer=optimizer,\n",
    "                                                                             teacher_forcing_ratio=0.5)\n",
    "\n",
    "character_accuracy.append(character_accuracy_list)\n",
    "sentence_accuracy.append(sentence_accuracy_list)\n",
    "\n",
    "torch.save(seq2seq.state_dict(), '../../../log/pth/palindrome_dedup_K100_even_single_Ctype3_parameter_no_att_model_save.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = list(range(1, 101, 1))\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(epochs[::3], character_accuracy[0][::3], '--', LineWidth=3, label=\"None Att\")\n",
    "plt.plot(epochs[::3], character_accuracy[1][::3], '-o', LineWidth=3, label=\"Luong Att\")\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=24)\n",
    "plt.ylabel('Character Accuracy', fontsize=24)\n",
    "plt.ylim([0, 1])\n",
    "plt.title('palindrome Ctype3', fontsize=35, fontweight=560)\n",
    "plt.savefig('../../../log/plot/palindrome_dedup_K100_even_single_Ctype3_parameter/epoch_to_character_accuracy.png')\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(epochs[::3], sentence_accuracy[0][::3], '--', LineWidth=3, label=\"None Att\")\n",
    "plt.plot(epochs[::3], sentence_accuracy[1][::3], '-o', LineWidth=3, label=\"Luong Att\")\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=24)\n",
    "plt.ylabel('Sentence Accuracy', fontsize=24)\n",
    "plt.ylim([0, 1])\n",
    "plt.title('palindrome Ctype3', fontsize=35, fontweight=560)\n",
    "plt.savefig('../../../log/plot/palindrome_dedup_K100_even_single_Ctype3_parameter/epoch_to_sentence_accuracy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(character_accuracy[0])\n",
    "print(character_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sentence_accuracy[0])\n",
    "print(sentence_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
